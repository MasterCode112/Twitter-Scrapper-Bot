Metadata-Version: 2.1
<<<<<<< HEAD
Name: Scrapper-Bot-Twitter
Version: 1.8
Summary: Tool for scraping Tweets
Home-page: https://github.com/Altimis/Scweet
Download-URL: https://github.com/Altimis/Scweet/archive/v0.3.0.tar.gz
Author: Yassine AIT JEDDI and Soufiane Bengadi
Author-email: aitjeddiyassine@gmail.com
License: MIT
Keywords: twitter,scraper,python,crawl,following,followers,twitter-scraper,tweets
=======
Name: Scrapper_Boot_Twitter
Version: 1.8
Summary: Tool for scraping Tweets
Home-page: https://github.com/MasterCode112/Scrapper_Boot_Twitter
Author: David Godbless Tenga
Author-email: dvdgodbless112@gmail.com
License: MIT
Download-URL: https://github.com/MasterCode112/Scrapper_Boot_Twitter/archive/v0.3.0.tar.gz
Keywords: twitter,scraper,python,crawl,following,followers,twitter-scraper,tweets
Platform: UNKNOWN
>>>>>>> 09010ee37567a780bc26f8a045371fea4428c40c
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development :: Build Tools
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.4
Classifier: Programming Language :: Python :: 3.5
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Description-Content-Type: text/markdown
<<<<<<< HEAD




# A simple and unlimited Twitter scraper with python.

Recently, Twitter has banned almost every Twitter scraper. This repository presents an alternative tool to scrape Twitter based on 3 functions:  
- [scrape](https://github.com/MasterCode112/Scrapper_Boot_Twitter/blob/master/Scrapper_Boot_Twitter/scrapper_boot.py): Scrapes all the information regarding tweets between two given dates, for a given language and list of words or account name, in the form of a csv file containing retrieved data (more storage methods will be added). 
- [get_user_information](https://github.com/MasterCode112/Scrapper_Boot_Twitter/blob/master/Scrapper_Boot_Twitter/user.py): Scrapes users information, incluing number of following and followers, location and description.
- [get_users_followers and get_users_following](https://github.com/MasterCode112/Scrapper_Boot_Twitter/blob/master/Scrapper_Boot_Twitter/user.py): Scrapes followers and following accounts for a given list of users.  

It is also possible to download the images showed in tweets by passing the argument `save_images = True`. If you only want to scrape images, it is recommended to set the argument `display_type = image` to show only tweets that contain images. 

Authentication is required for scraping followers/following. It is recommended to log in with a new account, otherwise the account could be banned if the list of followers is very long. To log in to your account, you need to enter your username `Scrapper_Boot_Twitter_USERNAME` and password `Scrapper_Boot_Twitter_PASSWORD` in the [.env](https://github.com/MasterCode112/Scrapper_Boot_Twitter/blob/master/.env) file. You can control the `wait` parameter in the `get_users_followers` and `get_users_following` functions according to you internet speed. 

## Requirements : 

`pip install -r requirements.txt`

Note : You must have Chrome installed on your system. 
=======
License-File: LICENSE.txt


# A simple and unlimited twitter scraper with python.

In the last days, Twitter banned almost every twitter scrapers. This repository represent an alternative tool to scrap tweets between two given dates (since and until), for a given language and list of words or account name, and saves a csv file containing retrieved data :  

``[UserScreenName,  UserName, Timestamp,  Text, Embedded_text, Emojis,  Comments, Likes,  Retweets, Image link, Tweet URL]``  

It is also possible to download and save the images from ``Image link`` by passing the argument ``save_images = True``, If you only want to scrape images, I recommand to set the argument ``display_type = image`` to show only tweets that contain images.  

You can scrape user profile information as well, including following and followers.  

Authentification is required in the case of followers/following scraping. It is recommended to log in with a new account (if the list of followers is very long, it is possible that your account will be banned). To log in to your account, you need to enter your username ``SCWEET_USERNAME`` and password ``SCWEET_PASSWORD`` in [env](https://github.com/Altimis/Scweet/blob/master/.env) file. You can controle the ``wait`` parameter in the ``get_users_followers`` and ``get_users_following`` functions. 

## Requierments : 

```pip install -r requirements.txt```

Note : You need to have Chrome installed in your system
>>>>>>> 09010ee37567a780bc26f8a045371fea4428c40c

## Results :

### Tweets :

The CSV file contains the following features (for each tweet) :

- 'UserScreenName' : 
- 'UserName' : UserName 
- 'Timestamp' : timestamp of the tweet
- 'Text' : tweet text
<<<<<<< HEAD
- 'Embedded_text' : embedded text written above the tweet. This can be an image, a video or even another tweet if the tweet in question is a reply
- 'Emojis' : emojis in the tweet
- 'Comments' : number of comments
- 'Likes' : number of likes
- 'Retweets' : number of retweets
- 'Image link' : link of the image in the tweet
- 'Tweet URL' : tweet URL

### Following / Followers :

The `get_users_following` and `get_users_followers` in [user](https://github.com/MasterCode112/Scrapper_Boot_Twitter/blob/master/Scrapper_Boot_Twitter/user.py) file give a list of following and followers for a given list of users.
=======
- 'Embedded_text' : embedded text written above the tweet. It could be an image, video or even another tweet if the tweet in question is a reply. 
- 'Emojis' : emojis existing in tweet
- 'Comments' : number of comments
- 'Likes' : number of likes
- 'Retweets' : number of retweets
- 'Image link' : Link of the image in the tweet
- 'Tweet URL' : Tweet URL.

### Following / Followers :

**More features will be added soon, such as "all reaplies of each tweet for a specific twitter account"**
>>>>>>> 09010ee37567a780bc26f8a045371fea4428c40c

## Usage :

### Library :

The library is now available. To install the library, run :

<<<<<<< HEAD
`pip install Scrapper_Boot_Twitter==1.8`

After the installation, you can import and use the functions as follows:

```
from Scrapper_Boot_Twitter.Scrapper_Boot_Twitter import scrape
from Scrapper_Boot_Twitter.user import get_user_information, get_users_following, get_users_followers
```

**Scrape top tweets with the words 'bitcoin', 'ethereum'  geolocated less than 200 km from Alicante (Spain) Lat=38.3452, Long=-0.481006 and without replies:**  
**The process is slower as the interval is smaller (choose an interval that can divide the period of time between, start and max date)**

```
data = scrape(words=['bitcoin','ethereum'], since="2021-10-01", until="2021-10-05", from_account = None,         interval=1, headless=False, display_type="Top", save_images=False, lang="en",
	resume=False, filter_replies=False, proximity=False, geocode="38.3452,-0.481006,200km")
```

**Scrape top tweets of with the hashtag #bitcoin, in proximity and without replies:**  
**The process is slower as the interval is smaller (choose an interval that can divide the period of time between, start and max date)**
=======
``pip install Scweet==1.6``

After installing, you can use it as follow : 

```
from Scrapper_Boot_Twitter.scrapper_boot import scrape
from Scrapper_Boot_Twitter.user import get_user_information, get_users_following, get_users_followers``
```

**scrape top tweets with the words 'bitcoin','ethereum'  geolocated less than 200 km from Alicante (Spain) Lat=38.3452, Long=-0.481006 and without replies.**  
**the process is slower as the interval is smaller (choose an interval that can divide the period of time betwee, start and max date)**

```
data = scrape(words=['bitcoin','ethereum'], since="2021-10-01", until="2021-10-05", from_account = None,         interval=1, headless=False, display_type="Top", save_images=False, lang="en",
  resume=False, filter_replies=False, proximity=False, geocode="38.3452,-0.481006,200km")
```

**scrape top tweets of with the hashtag #bitcoin, in proximity and without replies.**  
**the process is slower as the interval is smaller (choose an interval that can divide the period of time betwee, start and max date)**
>>>>>>> 09010ee37567a780bc26f8a045371fea4428c40c

```
data = scrape(hashtag="bitcoin", since="2021-08-05", until=None, from_account = None, interval=1, 
              headless=True, display_type="Top", save_images=False, 
              resume=False, filter_replies=True, proximity=True)
```

<<<<<<< HEAD
**Get the main information of a given list of users:**  
**These users follow me on Twitter**
=======
**Get the main information of a given list of users**  
**These users belongs to my following.**
>>>>>>> 09010ee37567a780bc26f8a045371fea4428c40c

```
users = ['nagouzil', '@yassineaitjeddi', 'TahaAlamIdrissi', 
         '@Nabila_Gl', 'geceeekusuu', '@pabu232', '@av_ahmet', '@x_born_to_die_x']
```

<<<<<<< HEAD
**This function will return a list that contains : **  
**["no. of following","no. of followers", "join date", "date of birth", "location", "website", "description"]**
=======
**this function return a list that contains : **  
**["nb of following","nb of followers", "join date", "birthdate", "location", "website", "description"]**
>>>>>>> 09010ee37567a780bc26f8a045371fea4428c40c

```
users_info = get_user_information(users, headless=True)
```

**Get followers and following of a given list of users**
<<<<<<< HEAD
**Enter your username and password in .env file. I recommend you do not use your main account.**  
**Increase wait argument to avoid banning your account and maximize the crawling process if the internet is slow. I used 1 and it's safe.**  

**Set your .env file with `Scrapper_Boot_Twitter_EMAIL` , `Scrapper_Boot_Twitter_USERNAME`  and `Scrapper_Boot_Twitter_PASSWORD` variables and provide its path**  
=======
**Enter your username and password in .env file. I recommande you dont use your main account.**  
**Increase wait argument to avoid banning your account and maximise the crawling process if the internet is slow. I used 1 and it's safe.**  

**set your .env file with SCWEET_EMAIL, SCWEET_USERNAME and SCWEET_PASSWORD variables and provide its path**  
>>>>>>> 09010ee37567a780bc26f8a045371fea4428c40c

```
env_path = ".env"

following = get_users_following(users=users, env=env_path, verbose=0, headless=True, wait=2, limit=50, file_path=None)

followers = get_users_followers(users=users, env=env_path, verbose=0, headless=True, wait=2, limit=50, file_path=None)
```

### Terminal :

```
Scrape tweets.

optional arguments:
  -h, --help            show this help message and exit
<<<<<<< HEAD
  --words WORDS         Words to search for. they should be separated by "//" : Cat//Dog.
=======
  --words WORDS         Words to search. they should be separated by "//" : Cat//Dog.
>>>>>>> 09010ee37567a780bc26f8a045371fea4428c40c
  --from_account FROM_ACCOUNT
                        Tweets posted by "from_account" account.
  --to_account TO_ACCOUNT
                        Tweets posted in response to "to_account" account.
  --mention_account MENTION_ACCOUNT
<<<<<<< HEAD
                        Tweets that mention "mention_account" account.         
  --hashtag HASHTAG
                        Tweets containing #hashtag
  --until UNTIL         End date for search query. example : %Y-%m-%d.
=======
                        Tweets mention "mention_account" account.         
  --hashtag HASHTAG
                        Tweets containing #hashtag
  --until UNTIL   max date for search query. example : %Y-%m-%d.
>>>>>>> 09010ee37567a780bc26f8a045371fea4428c40c
  --since SINCE
                        Start date for search query. example : %Y-%m-%d.
  --interval INTERVAL   Interval days between each start date and end date for
                        search queries. example : 5.
<<<<<<< HEAD
  --lang LANG           Tweets language. Example : "en" for english and "fr"
                        for french.
  --headless HEADLESS   Headless webdrives or not. True or False
  --limit LIMIT         Limit tweets to be scraped.
  --display_type DISPLAY_TYPE
                        Display type of Twitter page : Latest or Top tweets
=======
  --lang LANG           tweets language. Example : "en" for english and "fr"
                        for french.
  --headless HEADLESS   Headless webdrives or not. True or False
  --limit LIMIT         Limit tweets per <interval>
  --display_type DISPLAY_TYPE
                        Display type of twitter page : Latest or Top tweets
>>>>>>> 09010ee37567a780bc26f8a045371fea4428c40c
  --resume RESUME       Resume the last scraping. specify the csv file path.
  --proxy PROXY         Proxy server
  --proximity PROXIMITY Proximity
  --geocode GEOCODE     Geographical location coordinates to center the
                        search (), radius. No compatible with proximity
  --minreplies MINREPLIES
                        Min. number of replies to the tweet
  --minlikes MINLIKES   Min. number of likes to the tweet
  --minretweets MINRETWEETS
                        Min. number of retweets to the tweet
<<<<<<< HEAD
```

### To run the script :
`python scrapper_boot.py --words "Challenge//CTF" --to_account "MasterCode112"  --until 2020-01-05 --since 2020-01-01 --limit 10 --interval 1 --display_type Latest --lang="en" --headless True`
=======

### To execute the script : 
python scrapper_boot.py --words "excellente//car" --to_account "tesla"  --until 2020-01-05 --since 2020-01-01 --limit 10 --interval 1 --display_type Latest --lang="en" --headless True
```


>>>>>>> 09010ee37567a780bc26f8a045371fea4428c40c
